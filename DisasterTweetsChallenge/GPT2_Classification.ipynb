{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# Load libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as loading_bar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import torch.utils.data as data_helpers\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (7613, 5)\n",
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "Testing data shape: (3263, 4)\n",
      "   id keyword location                                               text\n",
      "0   0     NaN      NaN                 Just happened a terrible car crash\n",
      "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
      "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
      "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
      "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "train_data = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "eval_data = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n",
    "\n",
    "print(f'Training data shape: {train_data.shape}')\n",
    "print(train_data.head())\n",
    "\n",
    "print(f'Testing data shape: {eval_data.shape}')\n",
    "print(eval_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of NaN entries by column: \n",
      " id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Counts of NaN entries by column: \\n\",train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: (Tweet Info: (7613, 4), Labels: (7613,))\n",
      "-------------\n",
      "[[1 nan nan\n",
      "  'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all']\n",
      " [4 nan nan 'Forest fire near La Ronge Sask. Canada']]\n",
      "-------------\n",
      "Submission Data: (Tweet Info: (3263, 4))\n"
     ]
    }
   ],
   "source": [
    "# Split training data into features and labels\n",
    "\n",
    "train_x, train_y = train_data.to_numpy()[:, :-1], train_data.iloc[:, -1].to_numpy(np.int64)\n",
    "eval_x = eval_data.to_numpy()\n",
    "\n",
    "\n",
    "print(f'Training Data: (Tweet Info: {train_x.shape}, Labels: {train_y.shape})')\n",
    "print('-------------')\n",
    "print(train_x[0:2])\n",
    "print('-------------')\n",
    "print(f'Submission Data: (Tweet Info: {eval_x.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: (Tweet Info: (6851, 4), Labels: (6851,))\n",
      "Testing Data: (Tweet Info: (762, 4), Labels: (762,))\n",
      "Submission Data: (Tweet Info: (3263, 4))\n"
     ]
    }
   ],
   "source": [
    "# Split training data for 2-Fold Cross Validation\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.1)\n",
    "\n",
    "\n",
    "print(f'Training Data: (Tweet Info: {train_x.shape}, Labels: {train_y.shape})')\n",
    "print(f'Testing Data: (Tweet Info: {test_x.shape}, Labels: {test_y.shape})')\n",
    "print(f'Submission Data: (Tweet Info: {eval_x.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and configure tokenizer\n",
    "\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'cls_token': '[CLS]', 'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  84\n"
     ]
    }
   ],
   "source": [
    "# Determine a good max length for strings\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in train_x[:,3]:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent+\" [CLS]\", add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6851\n",
      "torch.Size([6851, 2, 100])\n",
      "762\n",
      "torch.Size([762, 2, 100])\n",
      "3263\n",
      "torch.Size([3263, 2, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10723,   278,   257,   582,  1742, 12204,  7241,   319,   262,  9753,\n",
       "           286,  1303,    76,  2178,   603,   420,   874,   318,  4753,   257,\n",
       "         11483,   492,  4380,  3387,  2193,  3228,  1303,  1203,   261,  1640,\n",
       "          6042,  1303,  3876, 20538,  1127,  1303,    76, 21645, 50257, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258],\n",
       "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize train_x, test_x, and eval_x\n",
    "\n",
    "def encode(tweet):\n",
    "    return tokenizer.encode_plus(\n",
    "        tweet,\n",
    "        add_special_tokens=True,\n",
    "        max_length = 100,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "\n",
    "encoded_train_x = [encode(tweet+\" [CLS]\") for tweet in train_x[:,3]]\n",
    "print(len(encoded_train_x))\n",
    "encoded_train_x = torch.cat([ \n",
    "    torch.reshape(\n",
    "        torch.cat((tweet['input_ids'], tweet['attention_mask'])), (1,2,-1)\n",
    "    ) \n",
    "    for tweet \n",
    "    in encoded_train_x \n",
    "])\n",
    "print(encoded_train_x.size())\n",
    "\n",
    "encoded_test_x = [encode(tweet+\" [CLS]\") for tweet in test_x[:,3]]\n",
    "print(len(encoded_test_x))\n",
    "encoded_test_x = torch.cat([ \n",
    "    torch.reshape(\n",
    "        torch.cat((tweet['input_ids'], tweet['attention_mask'])), (1,2,-1)\n",
    "    ) \n",
    "    for tweet \n",
    "    in encoded_test_x \n",
    "])\n",
    "print(encoded_test_x.size())\n",
    "\n",
    "encoded_eval_x = [encode(tweet+\" [CLS]\") for tweet in eval_x[:,3]]\n",
    "print(len(encoded_eval_x))\n",
    "encoded_eval_x = torch.cat([ \n",
    "    torch.reshape(\n",
    "        torch.cat((tweet['input_ids'], tweet['attention_mask'])), (1,2,-1)\n",
    "    ) \n",
    "    for tweet \n",
    "    in encoded_eval_x \n",
    "])\n",
    "print(encoded_eval_x.size())\n",
    "\n",
    "encoded_train_x[0]\n",
    "\n",
    "# Dim(encoded_train_x) = (Num Samples, 2, Length of Input_Ids/Attention_Mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Float (got Long)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ceef73111282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Store labels in tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Float (got Long)"
     ]
    }
   ],
   "source": [
    "# Store labels in tensors\n",
    "\n",
    "train_y = torch.Tensor(train_y).long()\n",
    "test_y = torch.Tensor(test_y).long()\n",
    "\n",
    "print(train_y.dtype, test_y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-32c0178bee8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define Custom Sequence Classification Model for GPT-2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGPT2ForSequenceClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPT2PreTrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformers' is not defined"
     ]
    }
   ],
   "source": [
    "# Define Custom Sequence Classification Model for GPT-2\n",
    "\n",
    "class GPT2ForSequenceClassification(transformers.GPT2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels \n",
    "        self.transformer = transformers.GPT2Model(config)\n",
    "        \n",
    "        model.config.hidden_dropout_prob = 0.1\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.n_embd, self.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        cls_token_ids=None,\n",
    "    ):\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past=past,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        \n",
    "        batch_size = cls_token_ids.shape[0]\n",
    "        cls_states = torch.cat([torch.reshape(transformer_outputs[0][i, cls_token_ids[i], :], (1, -1)) for i in range(batch_size)])\n",
    "\n",
    "        cls_states = self.dropout(cls_states)\n",
    "        logits = self.classifier(cls_states)\n",
    "\n",
    "        outputs = (logits,)\n",
    "\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6a2df5bd503d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m't'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'c'"
     ]
    }
   ],
   "source": [
    "a = {'t': 3}\n",
    "b = a['c'] or 4\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 100])\n",
      "torch.Size([4, 100])\n",
      "torch.Size([4, 100])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer and loss\n",
    "\n",
    "train_sampler = data_helpers.BatchSampler(\n",
    "            data_helpers.SubsetRandomSampler(range(train_x.shape[0])),\n",
    "            batch_size=4,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "test_sampler = data_helpers.BatchSampler(\n",
    "            data_helpers.SubsetRandomSampler(range(test_x.shape[0])),\n",
    "            batch_size=4,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "optimizer = transformers.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Test that sampler works properly\n",
    "for batch in train_sampler:\n",
    "    batch_input_ids = encoded_train_x[batch, 0, :]\n",
    "    batch_attention_mask = encoded_train_x[batch, 1, :]\n",
    "    batch_labels = train_y[batch]\n",
    "    \n",
    "    print(encoded_train_x[batch,:,:].size())\n",
    "    print(batch_input_ids.size())\n",
    "    print(batch_attention_mask.size())\n",
    "    print(len(batch_labels))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "# Move model and data to GPU if GPU is available\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    print(\"cuda is available\")\n",
    "    cuda = torch.device('cuda')  \n",
    "    model = model.to(cuda)\n",
    "    encoded_train_x = encoded_train_x.to(cuda)\n",
    "    train_y = train_y.to(cuda)\n",
    "    encoded_test_x = encoded_test_x.to(cuda)\n",
    "    test_y = test_y.to(cuda)\n",
    "    encoded_eval_x = encoded_eval_x.to(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9491b01041948cfbad580f1f8fb2f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='(Current Epoch: 0)', max=1713.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5d1ee498fa47e7a56560f97a6485fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='(Current Epoch: 1)', max=1713.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0df31b2d04643fc96eb4cf6a1c53d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='(Current Epoch: 2)', max=1713.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be7638317b8472f9e7400c99415d85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='(Current Epoch: 3)', max=1713.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "model.train()\n",
    "\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    for batch in loading_bar(train_sampler, desc=f'(Current Epoch: {epoch})'):\n",
    "        batch_input_ids = encoded_train_x[batch, 0, :]\n",
    "        batch_attention_mask = encoded_train_x[batch, 1, :]\n",
    "        batch_labels = train_y[batch]\n",
    "        batch_cls_token_ids = (batch_input_ids==tokenizer.cls_token_id).nonzero(as_tuple=True)[1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, logits = model(batch_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=batch_attention_mask,\n",
    "                             labels=batch_labels,\n",
    "                             cls_token_ids=batch_cls_token_ids)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f46f9e33cb74b67bbef1f01b48cf30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=191.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.8307086614173228\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0\n",
    "num_correct = 0\n",
    "for batch in loading_bar(test_sampler):\n",
    "    batch_input_ids = encoded_test_x[batch, 0, :]\n",
    "    batch_attention_mask = encoded_test_x[batch, 1, :]\n",
    "    batch_labels = test_y[batch]\n",
    "    batch_cls_token_ids = (batch_input_ids==tokenizer.cls_token_id).nonzero(as_tuple=True)[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss, logits = model(batch_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=batch_attention_mask,\n",
    "                             labels=batch_labels,\n",
    "                             cls_token_ids=batch_cls_token_ids)\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    MAP = torch.argmax(logits, dim=1)\n",
    "    num_correct += (batch_labels == MAP).sum().item()\n",
    "    \n",
    "accuracy = num_correct/test_x.shape[0]\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e171c17e2ee640d4a83907be09204a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3263.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6], device='cuda:0')\n",
      "tensor([15], device='cuda:0')\n",
      "tensor([22], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([17], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([35], device='cuda:0')\n",
      "tensor([17], device='cuda:0')\n",
      "tensor([41], device='cuda:0')\n",
      "tensor([63], device='cuda:0')\n",
      "tensor([33], device='cuda:0')\n",
      "tensor([15], device='cuda:0')\n",
      "tensor([27], device='cuda:0')\n",
      "tensor([40], device='cuda:0')\n",
      "tensor([45], device='cuda:0')\n",
      "tensor([24], device='cuda:0')\n",
      "tensor([33], device='cuda:0')\n",
      "tensor([34], device='cuda:0')\n",
      "tensor([50], device='cuda:0')\n",
      "tensor([21], device='cuda:0')\n",
      "tensor([40], device='cuda:0')\n",
      "tensor([32], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([47], device='cuda:0')\n",
      "tensor([33], device='cuda:0')\n",
      "tensor([28], device='cuda:0')\n",
      "tensor([38], device='cuda:0')\n",
      "tensor([35], device='cuda:0')\n",
      "tensor([29], device='cuda:0')\n",
      "tensor([35], device='cuda:0')\n",
      "tensor([42], device='cuda:0')\n",
      "tensor([31], device='cuda:0')\n",
      "tensor([41], device='cuda:0')\n",
      "tensor([29], device='cuda:0')\n",
      "tensor([40], device='cuda:0')\n",
      "tensor([35], device='cuda:0')\n",
      "tensor([24], device='cuda:0')\n",
      "tensor([64], device='cuda:0')\n",
      "tensor([72], device='cuda:0')\n",
      "tensor([39], device='cuda:0')\n",
      "tensor([75], device='cuda:0')\n",
      "tensor([19], device='cuda:0')\n",
      "tensor([35], device='cuda:0')\n",
      "tensor([35], device='cuda:0')\n",
      "tensor([72], device='cuda:0')\n",
      "tensor([15], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([23], device='cuda:0')\n",
      "tensor([26], device='cuda:0')\n",
      "tensor([25], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([33], device='cuda:0')\n",
      "tensor([39], device='cuda:0')\n",
      "tensor([11], device='cuda:0')\n",
      "tensor([37], device='cuda:0')\n",
      "tensor([33], device='cuda:0')\n",
      "tensor([36], device='cuda:0')\n",
      "tensor([27], device='cuda:0')\n",
      "tensor([34], device='cuda:0')\n",
      "tensor([28], device='cuda:0')\n",
      "tensor([49], device='cuda:0')\n",
      "tensor([15], device='cuda:0')\n",
      "tensor([39], device='cuda:0')\n",
      "tensor([36], device='cuda:0')\n",
      "tensor([36], device='cuda:0')\n",
      "tensor([35], device='cuda:0')\n",
      "tensor([24], device='cuda:0')\n",
      "tensor([36], device='cuda:0')\n",
      "tensor([43], device='cuda:0')\n",
      "tensor([26], device='cuda:0')\n",
      "tensor([38], device='cuda:0')\n",
      "tensor([23], device='cuda:0')\n",
      "tensor([30], device='cuda:0')\n",
      "tensor([22], device='cuda:0')\n",
      "tensor([33], device='cuda:0')\n",
      "tensor([38], device='cuda:0')\n",
      "tensor([48], device='cuda:0')\n",
      "tensor([47], device='cuda:0')\n",
      "tensor([32], device='cuda:0')\n",
      "tensor([54], device='cuda:0')\n",
      "tensor([51], device='cuda:0')\n",
      "tensor([31], device='cuda:0')\n",
      "tensor([34], device='cuda:0')\n",
      "tensor([23], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([23], device='cuda:0')\n",
      "tensor([35], device='cuda:0')\n",
      "tensor([20], device='cuda:0')\n",
      "tensor([32], device='cuda:0')\n",
      "tensor([47], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([26], device='cuda:0')\n",
      "tensor([12], device='cuda:0')\n",
      "tensor([18], device='cuda:0')\n",
      "tensor([40], device='cuda:0')\n",
      "tensor([49], device='cuda:0')\n",
      "tensor([26], device='cuda:0')\n",
      "tensor([40], device='cuda:0')\n",
      "tensor([21], device='cuda:0')\n",
      "tensor([30], device='cuda:0')\n",
      "tensor([38], device='cuda:0')\n",
      "tensor([39], device='cuda:0')\n",
      "tensor([19], device='cuda:0')\n",
      "tensor([40], device='cuda:0')\n",
      "tensor([18], device='cuda:0')\n",
      "tensor([38], device='cuda:0')\n",
      "tensor([39], device='cuda:0')\n",
      "tensor([36], device='cuda:0')\n",
      "tensor([39], device='cuda:0')\n",
      "tensor([39], device='cuda:0')\n",
      "tensor([25], device='cuda:0')\n",
      "tensor([39], device='cuda:0')\n",
      "tensor([39], device='cuda:0')\n",
      "tensor([40], device='cuda:0')\n",
      "tensor([39], device='cuda:0')\n",
      "tensor([19], device='cuda:0')\n",
      "tensor([40], device='cuda:0')\n",
      "tensor([51], device='cuda:0')\n",
      "tensor([39], device='cuda:0')\n",
      "tensor([28], device='cuda:0')\n",
      "tensor([4], device='cuda:0')\n",
      "tensor([32], device='cuda:0')\n",
      "tensor([17], device='cuda:0')\n",
      "tensor([39], device='cuda:0')\n",
      "tensor([36], device='cuda:0')\n",
      "tensor([23], device='cuda:0')\n",
      "tensor([31], device='cuda:0')\n",
      "tensor([25], device='cuda:0')\n",
      "tensor([37], device='cuda:0')\n",
      "tensor([37], device='cuda:0')\n",
      "tensor([50], device='cuda:0')\n",
      "tensor([11], device='cuda:0')\n",
      "tensor([50], device='cuda:0')\n",
      "tensor([22], device='cuda:0')\n",
      "tensor([23], device='cuda:0')\n",
      "tensor([23], device='cuda:0')\n",
      "tensor([28], device='cuda:0')\n",
      "tensor([28], device='cuda:0')\n",
      "tensor([27], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([44], device='cuda:0')\n",
      "tensor([29], device='cuda:0')\n",
      "tensor([25], device='cuda:0')\n",
      "tensor([41], device='cuda:0')\n",
      "tensor([49], device='cuda:0')\n",
      "tensor([42], device='cuda:0')\n",
      "tensor([42], device='cuda:0')\n",
      "tensor([41], device='cuda:0')\n",
      "tensor([52], device='cuda:0')\n",
      "tensor([26], device='cuda:0')\n",
      "tensor([41], device='cuda:0')\n",
      "tensor([41], device='cuda:0')\n",
      "tensor([41], device='cuda:0')\n",
      "tensor([33], device='cuda:0')\n",
      "tensor([41], device='cuda:0')\n",
      "tensor([41], device='cuda:0')\n",
      "tensor([49], device='cuda:0')\n",
      "tensor([30], device='cuda:0')\n",
      "tensor([24], device='cuda:0')\n",
      "tensor([27], device='cuda:0')\n",
      "tensor([33], device='cuda:0')\n",
      "tensor([26], device='cuda:0')\n",
      "tensor([46], device='cuda:0')\n",
      "tensor([47], device='cuda:0')\n",
      "tensor([40], device='cuda:0')\n",
      "tensor([47], device='cuda:0')\n",
      "tensor([29], device='cuda:0')\n",
      "tensor([30], device='cuda:0')\n",
      "tensor([49], device='cuda:0')\n",
      "tensor([28], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([25], device='cuda:0')\n",
      "tensor([38], device='cuda:0')\n",
      "tensor([43], device='cuda:0')\n",
      "tensor([47], device='cuda:0')\n",
      "tensor([48], device='cuda:0')\n",
      "tensor([38], device='cuda:0')\n",
      "tensor([21], device='cuda:0')\n",
      "tensor([22], device='cuda:0')\n",
      "tensor([29], device='cuda:0')\n",
      "tensor([36], device='cuda:0')\n",
      "tensor([12], device='cuda:0')\n",
      "tensor([51], device='cuda:0')\n",
      "tensor([36], device='cuda:0')\n",
      "tensor([31], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([36], device='cuda:0')\n",
      "tensor([32], device='cuda:0')\n",
      "tensor([22], device='cuda:0')\n",
      "tensor([25], device='cuda:0')\n",
      "tensor([32], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([36], device='cuda:0')\n",
      "tensor([23], device='cuda:0')\n",
      "tensor([33], device='cuda:0')\n",
      "tensor([42], device='cuda:0')\n",
      "tensor([38], device='cuda:0')\n",
      "tensor([35], device='cuda:0')\n",
      "tensor([32], device='cuda:0')\n",
      "tensor([35], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "\n",
    "predictions = []\n",
    "for i in loading_bar(range(len(encoded_eval_x))):\n",
    "    input_ids = encoded_eval_x[i, 0, :].reshape(1,-1)\n",
    "    attention_mask = encoded_eval_x[i, 1, :].reshape(1,-1)\n",
    "    cls_token_id = (input_ids==tokenizer.cls_token_id).nonzero(as_tuple=True)[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=attention_mask,\n",
    "                       cls_token_ids=cls_token_id)\n",
    "    \n",
    "    MAP = torch.argmax(logits[0]).item()\n",
    "    predictions.append(MAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Predictions\n",
    "\n",
    "output = pd.DataFrame({'id': eval_x[:,0], 'target': predictions})\n",
    "output.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
