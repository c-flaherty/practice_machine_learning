{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as loading_bar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import torch.utils.data as data_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "train_data = pd.read_csv(\"nlp-getting-started/train.csv\")\n",
    "eval_data = pd.read_csv(\"nlp-getting-started/test.csv\")\n",
    "\n",
    "print(f'Training data shape: {train_data.shape}')\n",
    "print(train_data.head())\n",
    "\n",
    "print(f'Testing data shape: {eval_data.shape}')\n",
    "print(eval_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if there are NaN's in text or target columns\n",
    "\n",
    "print(\"Counts of NaN entries by column: \\n\",train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into features and labels\n",
    "\n",
    "train_x, train_y = train_data.to_numpy()[:, :-1], train_data.iloc[:, -1].to_numpy(np.int64)\n",
    "eval_x = eval_data.to_numpy()\n",
    "\n",
    "\n",
    "print(f'Training Data: (Tweet Info: {train_x.shape}, Labels: {train_y.shape})')\n",
    "print(f'Submission Data: (Tweet Info: {eval_x.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data for 2-Fold Cross Validation\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.1)\n",
    "\n",
    "\n",
    "print(f'Training Data: (Tweet Info: {train_x.shape}, Labels: {train_y.shape})')\n",
    "print(f'Testing Data: (Tweet Info: {test_x.shape}, Labels: {test_y.shape})')\n",
    "print(f'Submission Data: (Tweet Info: {eval_x.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pre-trained tokenizer\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine a good max length for strings\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in train_x[:,3]:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that add_special_tokens adds [CLS] to start of sequence\n",
    "\n",
    "print(tokenizer.encode(\"HI my name is\", add_special_tokens=True))\n",
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.all_special_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize train_x, test_x, and eval_x\n",
    "\n",
    "def encode(tweet):\n",
    "    return tokenizer.encode_plus(\n",
    "        tweet,\n",
    "        add_special_tokens=True,\n",
    "        max_length = 120,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "\n",
    "encoded_train_x = [encode(tweet) for tweet in train_x[:,3]]\n",
    "print(len(encoded_train_x))\n",
    "encoded_train_x = torch.cat([ \n",
    "    torch.reshape(\n",
    "        torch.cat((tweet['input_ids'], tweet['attention_mask'])), (1,2,-1)\n",
    "    ) \n",
    "    for tweet \n",
    "    in encoded_train_x \n",
    "])\n",
    "print(encoded_train_x.size())\n",
    "\n",
    "encoded_test_x = [encode(tweet) for tweet in test_x[:,3]]\n",
    "print(len(encoded_test_x))\n",
    "encoded_test_x = torch.cat([ \n",
    "    torch.reshape(\n",
    "        torch.cat((tweet['input_ids'], tweet['attention_mask'])), (1,2,-1)\n",
    "    ) \n",
    "    for tweet \n",
    "    in encoded_test_x \n",
    "])\n",
    "print(encoded_test_x.size())\n",
    "\n",
    "encoded_eval_x = [encode(tweet) for tweet in eval_x[:,3]]\n",
    "print(len(encoded_eval_x))\n",
    "encoded_eval_x = torch.cat([ \n",
    "    torch.reshape(\n",
    "        torch.cat((tweet['input_ids'], tweet['attention_mask'])), (1,2,-1)\n",
    "    ) \n",
    "    for tweet \n",
    "    in encoded_eval_x \n",
    "])\n",
    "print(encoded_eval_x.size())\n",
    "\n",
    "encoded_train_x[0]\n",
    "\n",
    "# Dim(encoded_train_x) = (Num Samples, 2, Length of Input_Ids/Attention_Mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store labels in tensors\n",
    "\n",
    "train_y = torch.Tensor(train_y).long()\n",
    "test_y = torch.Tensor(test_y).long()\n",
    "\n",
    "print(train_y.dtype, test_y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "\n",
    "'''\n",
    "IDEA: I could create a custom classification head on BERT that takes as input the output of BERT model,\n",
    "         AND the location and keyword information\n",
    "'''\n",
    "model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    num_labels=2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss\n",
    "\n",
    "train_sampler = data_helpers.BatchSampler(\n",
    "            data_helpers.SubsetRandomSampler(range(train_x.shape[0])),\n",
    "            batch_size=4,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "test_sampler = data_helpers.BatchSampler(\n",
    "            data_helpers.SubsetRandomSampler(range(test_x.shape[0])),\n",
    "            batch_size=4,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "optimizer = transformers.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Test that sampler works properly\n",
    "for batch in sampler:\n",
    "    batch_input_ids = encoded_train_x[batch, 0, :]\n",
    "    batch_attention_mask = encoded_train_x[batch, 1, :]\n",
    "    batch_labels = train_y[batch]\n",
    "    \n",
    "    print(encoded_train_x[batch,:,:].size())\n",
    "    print(batch_input_ids.size())\n",
    "    print(batch_attention_mask.size())\n",
    "    print(len(batch_labels))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPA/CUDA is available, move model and data to GPU\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    print(\"cuda is available\")\n",
    "    cuda = torch.device('cuda')  \n",
    "    model = model.to(cuda)\n",
    "    encoded_train_x = encoded_train_x.to(cuda)\n",
    "    train_y = train_y.to(cuda)\n",
    "    encoded_test_x = encoded_test_x.to(cuda)\n",
    "    test_y = test_y.to(cuda)\n",
    "    encoded_eval_x = encoded_eval_x.to(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "model.train()\n",
    "\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    for batch in loading_bar(train_sampler, desc=f'(Current Epoch: {epoch})'):\n",
    "        batch_input_ids = encoded_train_x[batch, 0, :]\n",
    "        batch_attention_mask = encoded_train_x[batch, 1, :]\n",
    "        batch_labels = train_y[batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, logits = model(batch_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=batch_attention_mask,\n",
    "                             labels=batch_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0\n",
    "num_correct = 0\n",
    "for batch in loading_bar(test_sampler):\n",
    "    batch_input_ids = encoded_test_x[batch, 0, :]\n",
    "    batch_attention_mask = encoded_test_x[batch, 1, :]\n",
    "    batch_labels = test_y[batch]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss, logits = model(batch_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=batch_attention_mask,\n",
    "                             labels=batch_labels)\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    MAP = torch.argmax(logits, dim=1)\n",
    "    num_correct += (batch_labels == MAP).sum().item()\n",
    "    \n",
    "accuracy = num_correct/test_x.shape[0]\n",
    "print(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "\n",
    "predictions = []\n",
    "for i in loading_bar(range(len(encoded_eval_x))):\n",
    "    input_ids = encoded_eval_x[i, 0, :].reshape(1,-1)\n",
    "    attention_mask = encoded_eval_x[i, 1, :].reshape(1,-1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=attention_mask)\n",
    "    \n",
    "    MAP = torch.argmax(logits[0]).item()\n",
    "    predictions.append(MAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "\n",
    "output = pd.DataFrame({'id': eval_x[:,0], 'target': predictions})\n",
    "output.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 2061,   318,   534,  1438,    30, 50257, 50258, 50258, 50258, 50258,\n",
      "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])}\n",
      "['[CLS]', '[PAD]', '<|endoftext|>']\n",
      "[50257, 50258, 50256]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'cls_token': '[CLS]', 'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))  # Update the model embeddings with the new vocabulary size\n",
    "\n",
    "print(tokenizer.encode_plus(\"What is your name? [CLS]\", add_special_tokens=True, max_length=100, pad_to_max_length=True, return_attention_mask=True,return_tensors='pt'))\n",
    "\n",
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.all_special_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = transformers.GPT2Model.from_pretrained('gpt2')\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = transformers.GPT2.from_pretrained('bert-base-cased')\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5303,   616,  1438,   318, 50257]])\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(tokenizer.encode(\"hi my name is [CLS]\", add_special_tokens=True)).unsqueeze(0)\n",
    "print(a)\n",
    "b = model(a)\n",
    "print(b[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50259\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Disaster Tweets",
   "language": "python",
   "name": "tweets-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
